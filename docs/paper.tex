\documentclass[12pt,a4paper]{article}

% ====== PACKAGES ======
\usepackage[T2A]{fontenc}     
\usepackage[utf8]{inputenc}   
\usepackage[english,russian]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{tikz-cd}

% ====== PAGE SETTINGS ======
\geometry{top=2cm, bottom=2cm, left=2.5cm, right=2.5cm}

% ====== DOCUMENT ======
\begin{document}

\title{Снижение размерности латентного пространства в задачах управления}
\author{Автор}

\begin{abstract}
Задача восстановления аттрактора динамической системы является ключевой в анализе нелинейных систем, поскольку знание структуры аттрактора позволяет предсказывать эволюцию системы и определять её скрытые закономерности. В данной работе рассматривается случай, когда доступны два временных ряда, описывающих одно и то же состояние динамической системы, но полученных из разных источников измерений. Для обоих временных рядов строятся вложения задержек, после чего проводится восстановление гомеоморфизма между ними. Такой подход позволяет сопоставлять состояния системы по разнородным сигналам, например, восстанавливать положение руки человека на основе данных инерциального датчика (IMU).
\end{abstract}

\section*{Введение}

Во многих задачах анализа и управления динамическими системами исследователь располагает несколькими временными рядами, отражающими состояние объекта, но имеющими различную природу измерений. Однако напрямую сравнить такие сигналы зачастую невозможно, поскольку каждый из них представляет собой проекцию многообразия состояний системы на собственное измерительное пространство. Возникает задача восстановления взаимно однозначного соответствия между такими представлениями.

Классический подход к восстановлению динамики по наблюдениям основан на теореме Такенса \cite{takens1981detecting}, согласно которой аттрактор системы может быть корректно реконструирован в виде вложения в пространство задержек. Это позволяет восстанавливать топологическую структуру фазового пространства только по одному скалярному временному ряду. Методы реконструкции фазового пространства получили широкое применение при анализе хаотических систем \cite{ott1990controlling}, биологических сигналов \cite{glass1986dynamics}, а также в задачах адаптивного управления и идентификации \cite{sauer1991embedology}.

В данной работе рассматривается ситуация, когда имеется два временных ряда, соответствующих одной и той же динамической системе. Построение вложения задержек по каждому ряду даёт два топологически эквивалентных многомерных представления системы. Восстановление гомеоморфизма между ними позволяет сопоставлять состояния системы, измеренные разнородными сенсорами. Это особенно актуально в задачах, где необходимо оценивать скрытое или геометрическое состояние объекта по косвенным данным. Например, восстановление положения руки по сигналам IMU требует корректного отображения аттракторов, построенных по угловым скоростям и ускорениям, в пространственные координаты движения.

Таким образом, восстановление гомеоморфизма между пространсвами задержек двух временных рядов обеспечивает возможность интеграции разнородных данных, повышения точности оценки состояния системы и последующего использования полученного отображения в задачах предсказания, управления и распознавания движений.



\section*{Постановка задачи}

Рассмотрим гладкую динамическую систему, задаваемую потоком 
\(\varphi_t : \mathcal{M} \to \mathcal{M}\) на многообразии состояний \(\mathcal{M}\) 
размерности \(n\), и её фазовый аттрактор \(\mathcal{A}\subset \mathcal{M}\).
Предположим, что динамика наблюдается через две различные функции наблюдения  
\[
h^{(1)} : \mathcal{M} \to \mathbb{R}^{d_1}, \qquad
h^{(2)} : \mathcal{M} \to \mathbb{R}^{d_2},
\]
отражающие два разных типа измерений одной и той же системы. Тогда при дискретизации времени
наблюдения имеют вид
\[
s^{(k)}_t = h^{(k)}(\varphi_{t\Delta t}(x_0)) + \eta^{(k)}_t,\quad k=1,2,
\]
где \(\eta^{(k)}_t\) обозначает шум измерений.

Для каждого временного ряда строится вложения задержек. При фиксированных задержках \(\tau_k\)
и размерностях вложения \(m_k\):
\[
X^{(k)}_t = \big(s^{(k)}_t,\, s^{(k)}_{t-\tau_k},\,\dots,\,s^{(k)}_{t-(m_k-1)\tau_k}\big)
\in \mathbb{R}^{m_k d_k},\quad k=1,2.
\]
Это позволяет получить два реконструированных пространсва состояний системы:
\[
\widehat{\mathcal{A}}_1 = \{X^{(1)}_t\}, \qquad
\widehat{\mathcal{A}}_2 = \{X^{(2)}_t\}.
\]

Согласно теореме Такенса и её обобщениям, при типичных наблюдениях и достаточно большой
размерности вложения оба реконструированных аттрактора являются гомеоморфными
фазовому аттрактору \(\mathcal{A}\). Отсюда следует, что существует гомеоморфизм
\[
\Phi:\widehat{\mathcal{A}}_1 \to \widehat{\mathcal{A}}_2,
\]
который устанавливает взаимно-однозначное соответствие между состояниями системы,
наблюдаемыми через разные сенсоры (например, соответствие между IMU-сигналом и
положением руки в пространстве).

Для восстановления отображения \(\Phi\) используется автоэнкодерная параметризация:
\[
\begin{tikzcd}
\mathcal{A}_1 \arrow[dr, shift left=0.5ex, rightarrow] 
  \arrow[dr, shift right=0.5ex, leftarrow] 
& & &
\mathcal{A}_2 \arrow[dl, shift left=0.5ex, rightarrow] 
  \arrow[dl, shift right=0.5ex, leftarrow] \\
& U \arrow[r, leftrightarrow] &  V & \\
\end{tikzcd}
\]

где латентные пространства \(U\), \(V\) интерпретируется как приближённая реконструкция общего состояния системы. подобные низкоразмерные состояния существуют поскоьку для полной реконструкции фазового пространства системя размерность вложения задержет должна быть больше чем \(d' + 1\) где \(d'\) размерность фазового пространства \cite{Deyle2011}


\section*{Восстановление соответствия с помощью CCA}

Рассмотрим задачу восстановления соответствия между двумя временными рядами, описывающими одну и ту же динамическую систему. Пусть у нас есть два наблюдаемых ряда
\[
s^{(1)}_t \in \mathbb{R}^{d_1}, \quad s^{(2)}_t \in \mathbb{R}^{d_2}, \quad t = 1,\dots,N,
\]
из которых строятся вложения задержек
\[
X^{(1)}_t = (s^{(1)}_t, s^{(1)}_{t-\tau}, \dots, s^{(1)}_{t-(m_1-1)\tau}) \in \mathbb{R}^{m_1 d_1},
\]
\[
X^{(2)}_t = (s^{(2)}_t, s^{(2)}_{t-\tau}, \dots, s^{(2)}_{t-(m_2-1)\tau}) \in \mathbb{R}^{m_2 d_2}.
\]

\subsection*{Canonical Correlation Analysis}

CCA — это метод поиска линейных комбинаций двух наборов переменных, которые максимально коррелированы между собой. Для двух случайных векторов \(X \in \mathbb{R}^{p}\) и \(Y \in \mathbb{R}^{q}\) CCA ищет векторы \(w_x \in \mathbb{R}^p\) и \(w_y \in \mathbb{R}^q\), решающие задачу
\[
\max_{w_x, w_y} \mathrm{corr}(w_x^\top X, w_y^\top Y)
= \frac{w_x^\top \Sigma_{XY} w_y}{\sqrt{w_x^\top \Sigma_{XX} w_x} \sqrt{w_y^\top \Sigma_{YY} w_y}},
\]
где \(\Sigma_{XX}, \Sigma_{YY}, \Sigma_{XY}\) — ковариационные матрицы \(X\) и \(Y\).

Чтобы обеспечить независимость канонических компонент, вводятся ограничения:
\[
W_1^\top \Sigma_{XX} W_1 = I, \quad W_2^\top \Sigma_{YY} W_2 = I,
\]
где \(W_1 = [w_x^{(1)}, \dots, w_x^{(r)}]\), \(W_2 = [w_y^{(1)}, \dots, w_y^{(r)}]\).  
Эти ограничения делают канонические компоненты ортогональными и нормированными, что исключает дублирование информации в разных компонентах.

\subsection*{Применение CCA для восстановления соответствия}

После центрирования данных строятся матрицы
\[
X = \begin{bmatrix} (X^{(1)}_1)^\top \\ \vdots \\ (X^{(1)}_N)^\top \end{bmatrix}, \quad
Y = \begin{bmatrix} (X^{(2)}_1)^\top \\ \vdots \\ (X^{(2)}_N)^\top \end{bmatrix}.
\]

CCA решает задачу
\[
(W_1, W_2) = \arg\max_{W_1, W_2} \mathrm{corr}(X W_1, Y W_2),
\]
с ортогональными весами \(W_1^\top \Sigma_{XX} W_1 = I, \, W_2^\top \Sigma_{YY} W_2 = I\).  

После понижения размерности мы поличили векторные представления с максимезированной корреляцией, соответвекнно логично предположить, что теперь можно придлизить связь между \(U\), \(V\) через линейное отображение и получить связь междку пространствами

\[
Y = XW_1 A {W_2}^{-1}
\]

где \(A\) обучаемая матрица связи, а \({W_2}^-1\) псевдообратная матрица Пенроуза

\section*{Нелинейная аппроксимация гомеоморфизма с помощью архитектуры трансформер}

Классические линейные методы, такие как канонический корреляционный анализ, оказываются недостаточными для восстановления сложных нелинейных зависимостей, возникающих в реальных данных. Для решения этой задачи предлагается использовать архитектуру на основе трансформеров, способную моделировать как локальные, так и глобальные временные зависимости благодаря механизму внимания.

Идея заключается в том, чтобы научиться отображать траектории из одного реконструированного фазового пространства в другое, сохраняя при этом топологическую структуру исходного аттрактора системы.

Трансформеры подходят для этой задачи по нескольким причинам. Механизм самовнимания позволяет учитывать глобальные временные зависимости внутри каждого аттрактора, что критически важно для сохранения динамических инвариантов системы. Нелинейные преобразования внутри архитектуры способны аппроксимировать сложные гомеоморфизмы \cite{transf}. Кроме того, модель может автоматически выявлять значимые временные масштабы в данных без явного задания временных окон.

\subsection*{Архитектурное решение}

Предлагаемая архитектура реализует принцип перевода между различными представлениями одного динамического процесса. Два независимых трансформер-энкодера работают параллельно, преобразуя последовательности из соответствующих вложени задержек в латентные представления:

\[
U = \mathcal{E}_1(X^{(1)}), \quad V = \mathcal{E}_2(X^{(2)})
\]

Ключевая идея заключается в том, что если оба временных ряда действительно описывают одну и ту же динамическую систему, то в латентном пространстве должна существовать простая связь между их представлениями. Мы предполагаем, что эта связь может быть аппроксимирована линейным преобразованием:

\[
\widetilde{V} =  U A
\]

где \(A\) — обратимая матрица, обеспечивающая структурное соответствие между латентными пространствами.

Затем два трансформер-декодера восстанавливают исходную структуру пространств:

\[
\hat{X}^{(1)} = \mathcal{D}_1(U), \quad \hat{X}^{(2)} = \mathcal{D}_2(V)
\]

Это необходимо для поддреживания обратимости отображения.

Таким образом оптимизационная задача формируется следуюющим образом:

\[
\min_{\theta} \; \mathcal{L}(\theta, \ A)
\]

где функция потерь определяется как

\[
\mathcal{L}(\theta, \ A) = 
\left\| \mathcal{E}_1(X_i; \theta) A - \mathcal{E}_2(Y_i; \theta) \right\|_2^2 
+ \left\| \mathcal{D}_1(\mathcal{E}_1(X_i; \theta) A; \theta) - X_i \right\|_2^2
+ \left\| \mathcal{D}_2(\mathcal{E}_2(Y_i; \theta) A; \theta) - Y_i \right\|_2^2
\]




где \(X_i\), \(Y_i\) соответствующие временные промежутки

\subsection*{Интерпретация и преимущества}

Энкодеры обучаются находить такие латентные представления, в которых сложный гомеоморфизм между исходными аттракторами сводится к линейному преобразованию. По сути, модель самостоятельно обнаруживает естественные координаты динамической системы, в которых различные измерения становятся согласованными.

Преимущество использования трансформеров перед другими архитектурами заключается в их способности учитывать много масштабные временные зависимости. Для динамических систем это особенно важно, поскольку характерные времена процессов могут существенно различаться. Механизм внимания позволяет модели автоматически определять, какие временные масштабы наиболее значимы для установления соответствия между аттракторами.

Обучение такой модели не требует парных примеров правильного соответствия между состояниями системы — достаточно иметь синхронизированные по времени ряды измерений. Модель самостоятельно обнаруживает скрытые инварианты динамики, что делает подход особенно ценным для задач, где прямое измерение соответствия невозможно или затруднительно.

Таким образом, предложенная архитектура не просто решает задачу восстановления отображения между разнородными измерениями, но и предоставляет мощный инструмент для анализа скрытой структуры динамических систем через их различные наблюдаемые проявления.



\section*{Геометрический подход с использованием многообразия SPD-матриц}

\subsection*{Мотивация: геометрическая структура фазового пространства}

В контексте восстановления взаимно-однозначного отображения между реконструированными аттракторами 
$\widehat{\mathcal{A}}_1$ и $\widehat{\mathcal{A}}_2$, особый интерес представляет анализ геометрии
динамики. При наблюдении сигналов через многоканальные сенсоры (например, EEG), траектории системы в фазовом 
пространстве обладают сложной топологией и кривизной. Пространственные ковариационные матрицы, вычисленные 
на скользящих временных окнах, несут информацию о локальной геометрии этих траекторий, отражая взаимодействие 
между каналами, топологию и амплитудную структуру сигнала.

Так как ковариационные матрицы являются симметричными положительно определёнными (SPD), 
они естественным образом принадлежат многообразию SPD-матриц 
$\mathcal{S}_{++}^d$, которое представляет собой риманово многообразие отрицательной кривизны \cite{Frstner2003AMF, Moakher2005}. 
В отличие от евклидовых пространств, в SPD-пространстве геодезические, расстояния и средние значения 
определяются с учётом его криволинейной структуры. Это позволяет точнее описывать топологию 
и повышает устойчивость к шуму и выбросам \cite{a7b3ae3f95344ceaafd558942ea852b3, Lu2025}.

\subsection*{Геометрическая интерпретация ковариационных матриц}

Рассмотрим временное окно сигнала:
\[
W_t = \{s_{t}, s_{t-1}, \dots, s_{t-w+1}\} \in \mathbb{R}^{w \times d},
\]
где $w$ — длина окна, $d$ — количество каналов. 
Ковариационная матрица
\[
C_t = \text{Cov}(W_t) = \frac{1}{w-1} \sum_{i=1}^{w} (s_i - \bar{s})(s_i - \bar{s})^\top
\in \mathbb{R}^{d \times d}
\]
является SPD-матрицей и описывает локальную геометрию фазового аттрактора в момент времени $t$.

Собственные векторы $C_t$ определяют направления максимальной изменчивости сигнала — 
локальные оси фазового пространства, а собственные значения характеризуют интенсивность изменений 
вдоль этих направлений, что можно интерпретировать как локальную кривизну траектории. 
Таким образом, последовательность $\{C_t\}$ кодирует геометрию потока $\varphi_t$.

\subsection*{Многообразие SPD-матриц и метрические свойства}

Пространство SPD-матриц определяется как
\[
\mathcal{S}_{++}^d = \{ S \in \mathbb{R}^{d\times d} \mid S = S^\top,\, x^\top S x > 0,\, \forall x \in \mathbb{R}^d \},
\]
и образует гладкое многообразие размерности $d(d+1)/2$. На нём можно ввести различные римановы метрики. 
Можно определить несколько мер расстояния между двумя симметричными положительно определёнными матрицами $S_1$ и $S_2$, которые, как правило, зависят от длины геодезической 
кривой, соединяющей $S_1$ и $S_2$ на римановом многообразии. 
Часто в задачах интерфейсов мозг–компьютер (BCI) используется аффинно-инвариантная метрика. 
Однако далее мы приводим математическую формулировку, основанную на лог-евклидовой метрике \cite{Arsigny2007}. 
Эта формулировка снижает вычислительные затраты, связанные с аффинно-инвариантным подходом, 
при этом сохраняя надёжные теоретические свойства \cite{Arsigny2007}.

Пространство симметричных положительно определённых (SPD) матриц может быть наделено 
структурой группы Ли. Для более глубокого понимания см. \cite{Arsigny2007}. 
В таком случае можно определить метрику расстояния между двумя SPD-матрицами 
$S_1$ и $S_2$ как бивариантную метрику на группе Ли SPD-матриц.

\[
d_{\text{LE}}(P_1, P_2) = \| \log(P_1) - \log(P_2) \|_F,
\]
которая интерпретирует SPD-пространство как евклидово в логарифмическом домене, 
сохраняя при этом ключевые инвариантные свойства. 
Такой подход позволяет эффективно вычислять геодезические, средние и отображения, 
а также реализовывать нейросетевые преобразования в SPD-пространстве.

\subsection*{SPDNet и геометрическое представление сигналов}

Huang и Van~Gool~\cite{Huang2017} предложили нейросетевую архитектуру \emph{SPDNet}, 
работающую напрямую на многообразии SPD-матриц. 
Она состоит из трёх основных типов слоёв, обеспечивающих нелинейное отображение данных 
в римановом пространстве:

\textbf{BiMap слой (билинейное отображение):}
    \[
    Z_k = W_k Z_{k-1} W_k^\top,
    \]
    где $W_k \in \text{St}(d_k, d_{k-1})$ — параметр на компактном многообразии Стифеля. 
    Этот слой аналогичен свёрточному, но сохраняет SPD-структуру и инвариантность к линейным преобразованиям.

\textbf{ReEig слой (нелинейность):}
    \[
    Z_k = U_{k-1} \max(\varepsilon I, \Sigma_{k-1}) U_{k-1}^\top,
    \]
    где $Z_{k-1} = U_{k-1}\Sigma_{k-1}U_{k-1}^\top$. 
    Слой устраняет малые собственные значения, повышая численную устойчивость.

\textbf{LogEig слой (переход в евклидово пространство):}
    \[
    Z_k = U_{k-1} \log(\Sigma_{k-1}) U_{k-1}^\top,
    \]
    что соответствует логарифмическому отображению SPD-матриц: здесь нет обучаемых параметров, прадставленные матрицы являются частью сингулярного разложения SPD матрицы. 
    После этого шага данные могут обрабатываться классическими MLP-модулями.

Таким образом блок SPDNet выглядит как \(f_{LogEig} \circ f_{ReEig} \circ f_{BiMap} \circ \ldots \circ f_{ReEig} \circ f_{BiMap}(C) \)

SPDNet позволяет изучать геометрию многоканальных сигналов, сохраняя топологическую структуру и 
инвариантность к шуму и масштабным искажениям. 
Последующие работы~\cite{Suh2021, a7b3ae3f95344ceaafd558942ea852b3, Wang2023, Ju2022TensorCSPNetAN, Wang2023, Li2023SPDDDPMDD} расширили SPDNet, 
вводя механизмы внимания, резидуальные блоки и комбинированные евклидово-SPD архитектуры.

\subsection*{Использование SPDNet дял построения соответсвия геометрических декрипторов}

Поскольку качественно восстановить сигнал по ковариацинной матрице добольно сложно мы предлогаем рассматреть задачу построения модели соответсвия на множестве геометрических дескрипторов аттракторов.

\\

Пусть \( \mathcal{A}_{C^{(1)}} \) траектория ковариационных матриц первого сигнала, а \( \mathcal{A}_{C^{(2)}} \) второго, тогда необходимо постоить отображение \(f :\mathcal{A}_{C^{(1)}} \rightarrow \mathcal{A}_{C^{(2)}}  \), для этого  воспользуемя архитектурой SPDNet для построения енкодер декодер модели. Если енкодер понижал размерность матрицы то декодер должен ее повышать:

\[
\theta^{\star} = 
\underset{\theta = \{\theta_{\mathcal{E}_1}, \theta_{\mathcal{E}_2}, 
\theta_{\mathcal{D}_1}, \theta_{\mathcal{D}_2}, \theta_{MLP}\}}{\arg\min} \;
\mathcal{L}(C_i^{(1)}, C_i^{(2)}; \theta)
\]

\[
\begin{aligned}
\mathcal{L}(C_i^{(1)}, C_i^{(2)}; \theta) = &
\; ||
MPL_{\theta_{MLP}}(\log(\mathcal{E}_1^{(\theta_{\mathcal{E}_1})}(C_i^{(1)})))
- \log(C_i^{(2)})||_2
\\
& + d_{LE}\!\big(
\mathcal{D}_1^{(\theta_{\mathcal{D}_1})}
(\mathcal{E}_1^{(\theta_{\mathcal{E}_1})}(C_i^{(1)})),
\, C_i^{(1)}\big)
+ d_{LE}\!\big(
\mathcal{D}_2^{(\theta_{\mathcal{D}_2})}
(\mathcal{E}_2^{(\theta_{\mathcal{E}_2})}(C_i^{(2)})),
\, C_i^{(2)}\big) + 
\end{aligned}
\]

\section*{Использование граф-диффузных моделей для приближения гомеоморфизма}


\subsection*{Уравнение диффузии}

Нас интересует изучение процессов диффузии на области $\Omega$. Неформально, диффузия описывает перемещение вещества из областей с более высокой концентрацией в области с более низкой концентрацией. Например, когда горячий объект помещается на холодную поверхность, тепло будет диффундировать от объекта к поверхности до тех пор, пока температуры не сравняются.

Пусть $x(t)$ обозначает семейство скалярных функций на $\Omega \times [0, \infty)$, представляющих распределение некоторого свойства (для простоты будем считать это температурой) на $\Omega$ в момент времени $t$, а $x(u,t)$ — его значение в точке $u \in \Omega$ в момент времени $t$. Согласно закону Фурье для теплопроводности, тепловой поток

\[
h = -g \nabla x,
\]

пропорционален градиенту температуры $\nabla x$, где $g$ — диффузивность, описывающая теплопроводные свойства $\Omega$. Идеализированная однородная постановка предполагает, что $g$ является постоянной скалярной величиной по всей области $\Omega$. Более общо, диффузивность может быть неоднородной (зависящей от позиции) функцией, которая может быть скалярной (в этом случае она просто масштабирует градиент температуры и является изотропной) или матричной (в этом случае диффузия называется анизотропной, т.е. зависящей от направления).  

Условие сохранения

\[
x_t = -\mathrm{div}(h)
\]

(в грубом приближении это означает, что изменения температуры происходят только за счёт теплового потока, измеряемого оператором дивергенции, т.е. тепло не создаётся и не уничтожается) приводит к уравнению в частных производных, называемому \emph{уравнением диффузии (тепла)}:

\[
\frac{\partial x(u,t)}{\partial t} = \mathrm{div}[g(u, x(u,t), t)\, \nabla x(u,t)],
\]

с начальным условием $x(u,0) = x_0(t)$. Для простоты мы предполагаем отсутствие граничных условий.  

Выбор функции диффузивности определяет, является ли диффузия однородной ($g = c$), неоднородной ($g(u,t)$) или анизотропной ($A(u,t)$). В изотропном случае уравнение диффузии можно записать как

\[
\frac{\partial x(u,t)}{\partial t} = \mathrm{div}(c \nabla x) = c \Delta x,
\]

где $\Delta x = \mathrm{div}(\nabla x)$ — оператор Лапласа.

\subsection*{Диффузия на многообразиях}

До этого момента мы предполагали некоторую абстрактную область $\Omega$. Структура области проявляется в определении пространственных дифференциальных операторов в уравнении диффузии. В общем случае мы моделируем $\Omega$ как риманово многообразие, и пусть $X(\Omega)$ и $X(T\Omega)$ обозначают пространства скалярных и (касательных) векторных полей на нём, соответственно.  

Обозначим через $\langle x, y \rangle$ и $\langle\langle X, Y \rangle\rangle$ соответствующие скалярные произведения на $X(\Omega)$ и $X(T\Omega)$. Далее обозначим через $\nabla : X(\Omega) \to X(T\Omega)$ и $\mathrm{div} = \nabla^* : X(T\Omega) \to X(\Omega)$ операторы градиента и дивергенции, которые являются сопряжёнными относительно указанных скалярных произведений:

\[
\langle\langle \nabla x, X \rangle\rangle = \langle x, \mathrm{div}(X) \rangle.
\]

Неформально, градиент $\nabla x$ скалярного поля $x$ является векторным полем, предоставляющим в каждой точке $u \in \Omega$ направление $\nabla x(u)$ наибольшего изменения $x$. Дивергенция $\mathrm{div}(X)$ векторного поля $X$ является скалярным полем, которое в каждой точке измеряет поток $X$ через бесконечно малый объём. Оператор Лапласа $\Delta x$ можно интерпретировать как локальную разницу между значением скалярного поля $x$ в точке и его окрестностью.

\subsection*{Диффузионное уравнение на графе}

Пусть $G = (V,E)$ — неориентированный граф с $|V| = n$ узлами и $|E| = e$ рёбрами, и пусть $x$ и $X$ обозначают признаки, определённые на узлах и рёбрах соответственно. Узловые и рёберные поля могут быть представлены как $n$- и $e$-мерные векторы, предполагая некоторый произвольный порядок узлов. Мы используем те же обозначения для скалярных произведений:

\[
\langle x, y \rangle = \sum_{i \in V} x_i y_i, \quad
\langle\langle X, Y \rangle\rangle = \sum_{i>j} w_{ij} X_{ij} Y_{ij},
\]

где $w_{ij}$ обозначает смежность графа: $w_{ij} = w_{ji} = 1$ тогда и только тогда, когда $(i,j) \in E$. Мы предполагаем, что рёберные поля чередуются, так что $X_{ji} = -X_{ij}$, и нет самосвязей, $(i,i) \notin E$.  

Градиент $(\nabla x)_{ij} = x_j - x_i$ присваивает ребру $(i,j) \in E$ разность признаков его концов и по определению чередуется. Аналогично, дивергенция

\[
(\mathrm{div}(X))_i = \sum_{j:(i,j) \in E} X_{ij} = \sum_{j=1}^n w_{ij} X_{ij}
\]

присваивает узлу $i$ сумму признаков всех рёбер, которыми он обладает. Два оператора являются сопряжёнными: $\langle\langle \nabla x, X \rangle\rangle = \langle x, \mathrm{div}(X) \rangle$.

Рассмотрим следующее диффузионное уравнение на графе:

\[
\frac{\partial x(t)}{\partial t} = \mathrm{div}[G(x(t),t) \nabla x(t)]
\]

с начальным условием $x(0)$. Здесь $G = \mathrm{diag}(a(x_i(t), x_j(t), t))$ — $e \times e$ диагональная матрица, а $a$ — функция, определяющая сходство между узлами $i$ и $j$. В общем случае $a(x_i, x_j, t)$ может зависеть от времени, но для простоты будем считать $a = a(x_i, x_j)$.

Подставляя выражения для $\nabla$ и $\mathrm{div}$, получаем:

\[
\frac{\partial}{\partial t} x(t) = (A(x(t)) - I)x(t) = \bar{A}(x(t)) x(t),
\]

где $A(x) = (a(x_i,x_j))$ — $n \times n$ матрица внимания с той же структурой, что и смежность графа ($a_{ij}=0$, если $(i,j) \notin E$). Заметим, что если $A(x(t)) = A$, то получаем линейное диффузионное уравнение, которое можно решить аналитически:

\[
x(t) = e^{\bar{A} t} x(0).
\]


\subsection*{Graph Neural Diffusion (GRAND)} 

Архитектуры GRAND реализует обучаемый процесс диффузии на графе, чтобы получить вложеня узлов узлов:

\[
Y = \psi(X(T)), \quad 
X(T) = X(0) + \int_0^T \frac{\partial X(t)}{\partial t} dt, \quad 
X(0) = \phi(X_\mathrm{in}),
\]

где $\frac{\partial X(t)}{\partial t}$ задаётся диффузионным уравнением на графе. Различные архитектуры GRAND отличаются выбором обучаемой функции диффузивности $G$ и пространственно-временной дискретизацией уравнения.

Диффузивность моделируется с помощью функции внимания $a(\cdot,\cdot)$. Эмпирически, dot-product attention  \cite{att} превосходит внимание GAT \cite{GAT}.  

\[
a(X_i, X_j) = \mathrm{softmax} \left( \frac{(W_K X_i)^\top W_Q X_j}{d_k} \right),
\]

где $W_K$ и $W_Q$ — обучаемые матрицы, а $d_k$ — гиперпараметр, определяющий размерность $W_K$.  

Используется \emph{multi-head attention}, которая полезна для стабилизации обучения \cite{GAT, att} с помощью усреднения по головам:

\[
A(X) = \frac{1}{h} \sum_{h} A_h(X).
\]

Матрица весов внимания $A = (a(X_i, X_j))$ является правостохастической, что позволяет записать диффузионное уравнение на графе в виде:

\[
\frac{\partial X}{\partial t} = (A(X) - I) X = \bar{A}(X) X.
\]


согласно первоисточнику \cite{GRAND} модель удовлетворяет условию Пикара–Линделёфа и потому приближает гомеоморфизм.
Таким образом аналогично трансформенной архитектуре мы можем задать следующую оптимизационную задачу:

\[
\min_{\theta} \; \mathcal{L}(\theta, \ A)
\]

\[
\mathcal{L}(\theta, \ A) = 
\left\| \mathcal{E}_1(X_i; \theta) A - \mathcal{E}_2(Y_i; \theta) \right\|_2^2 
+ \left\| \mathcal{D}_1(\mathcal{E}_1(X_i; \theta) A; \theta) - X_i \right\|_2^2
+ \left\| \mathcal{D}_2(\mathcal{E}_2(Y_i; \theta) A; \theta) - Y_i \right\|_2^2
\]

\section*{Использование скрытого состояния для управления  ????}
Мы только что рассмотрели множество методов построения латентного состояния системы для различных постановок задачи, а так же процес диффузии который приближает нужный нам гомеоморфизм, логино поставить цель объединить рассмотренные методы с диффузным процессом, поскольку они используют разные представления и идеи в себе, мы считаем что самым логичным будет объединить GRAND вместе с учетом геометрии при помощи риманового многообразия SPD поскольку оба методя являются наилучшими на текущий момент, например SPD многообразие позволяет создать латентное предствение данных для качественной классификации состояний динамической системы, например детекции походки человека или положения рук \cite{Barachant2012}.


Поскольку мы качественно научились приближать скрытые состояния двух аттракторов систем, логично использовать это как условия для управления диффузие GRAND: давайте рассмотрим следующую постановку


Пусть у нас есть динамическая система, наблюдаемая через многоканальные сигналы, собранные в матрицу $X \in \mathbb{R}^{n \times d}$, где каждая строка соответствует вектору канала. Наша цель — построить латентное представление системы, которое отражает ключевые состояния динамики, например, позу человека или походку, и использовать это представление для управления диффузионным процессом на графе.


\subsection*{Диффузионный процесс на графе (GRAND)}

Пусть задан граф $G = (V,E)$, где узлы соответствуют компонентам системы, а рёбра отражают связи между ними. Обозначим $X(t) \in \mathbb{R}^{n \times d}$ как матрицу признаков узлов в момент времени $t$. Диффузионный процесс задаётся уравнением:

\[
\frac{\partial X(t)}{\partial t} = \mathrm{div}\big[ G(X(t),t) \nabla X(t) \big], \quad X(0) = \phi(X_\mathrm{in}),
\]

где $G(X,t)$ — обучаемая функция диффузивности (например, attention), а $\phi$ — обучаемый энкодер.

\subsection*{Детерминированная задача управления GRAND с внешним латентным представлением}

Пусть $X(t) \in \mathbb{R}^{n \times d}$ — состояние системы в момент времени $t$, где $n$ — число каналов, $d$ — размер признаков каждого канала. Латентное представление определяется внешней функцией
\[
z(t) = f_\theta(X(t)),
\]
и известны начальное и целевое латентные состояния $z_0$ и $z_T$.

\subsubsection*{Динамика системы с графовой диффузией}

Детерминированная эволюция системы с управляющим воздействием $U(t) \in \mathbb{R}^{n \times d}$ задается как
\[
\frac{dX(t)}{dt} = \bar{A}(X(t))X(t) + U(t), \quad X(0) = X_0,
\]

\subsubsection*{Функционал оптимизации}

Задача управления формулируется как минимизация функционала:
\[
\min_{U(t)} \; J(U) = d_\mathcal{M}\big(z(T), z_T\big)^2 + \lambda \int_0^T \|U(t)\|_F^2 \, dt,
\]
где $\|\cdot\|_F$ — норма Фробениуса, $\lambda > 0$ — коэффициент штрафа за энергию управления, $d_\mathcal{M}$ — метрика в латентном пространстве.


\section*{Эксперименты}

Проведем эксперименты на синтетических данных: возьмем аттрактор Лоренца и для одной функции наблюдения возьмем первую компоненту, а для второй вторую, таким образом мы соблюдим условия задачи и проанализируем модели, также послкольку аттрактор Лоренса имеет трехмерную размерноть можно будет оценить сохранения геометрической структуры.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{latent_alignment_3d.png}
    \caption{Латентное состояние построенное CCA}
    \label{fig:example}
\end{figure}

\begin{figure}[htbp] 
    \centering        
    \includegraphics[width=1.0\textwidth]{latent_trajectories_side_by_side.png} 
    \caption{Латентное состояние построенное Transformer}
    \label{fig:example}
\end{figure}

\begin{figure}[htbp]
    \centering 
    \includegraphics[width=1.0\textwidth]{covariance_heatmaps.png}
    \caption{полученное латентное представление на SPD многообразии}
    \label{fig:example}
\end{figure}

\begin{figure}[htbp]
    \centering 
    \includegraphics[width=1.0\textwidth]{latent_trajectories_all_windows.png}
    \caption{полученное латентное представление с помощью GRAND}
    \label{fig:example}
\end{figure}

\newpage

\bibliographystyle{plain}
\bibliography{refs}


\end{document}