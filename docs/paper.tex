\documentclass[12pt,a4paper]{article}

% ====== PACKAGES ======
\usepackage[T2A]{fontenc}     
\usepackage[utf8]{inputenc}   
\usepackage[english,russian]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{tikz-cd}

% ====== PAGE SETTINGS ======
\geometry{top=2cm, bottom=2cm, left=2.5cm, right=2.5cm}

% ====== DOCUMENT ======
\begin{document}

\title{Снижение размерности латентного пространства в задачах управления}
\author{Автор}
\date{}
\maketitle

\begin{abstract}
Задача восстановления фазового аттрактора динамической системы по наблюдаемым временным рядам является фундаментальной в анализе нелинейных систем, поскольку структура аттрактора определяет ключевые свойства динамики и позволяет строить предсказательные модели. В данной работе рассматривается задача реконструкции аттрактора с использованием вложения задержек в смысле теоремы Такенса. Построенное таким образом пространство состояний, как правило, имеет избыточную размерность, существенно превосходящую истинную размерность динамической системы. Целью работы является понижение размерности реконструированного аттрактора при сохранении всей информации о динамике системы. Предлагаемый подход позволяет получить компактное представление фазового пространства, сохраняющее его топологическую структуру и пригодное для последующего анализа, моделирования и предсказания поведения динамической системы.
\end{abstract}

\section*{Введение}

Восстановление фазового пространства динамической системы по наблюдаемым временным рядам является ключевой задачей анализа нелинейных систем. Знание структуры аттрактора позволяет исследовать динамику системы, выявлять скрытые закономерности и строить предсказательные модели. Классический подход к реконструкции фазового пространства основан на теореме Такенса \cite{takens1981detecting}, которая утверждает, что аттрактор системы можно корректно восстановить с помощью вложения задержек одного скалярного временного ряда, и реконструированное вложение сохраняет топологическую структуру оригинального фазового пространства — имеется дифференцируемое вложение, эквивалентное аттрактору системы \cite{takens1981detecting,turn0search27,turn0search9}.

На практике построение вложения задержек приводит к пространству, размерность которого значительно превышает истинную размерность динамической системы ввиду необходимости удовлетворения условиям теоремы Такенса (например, $p \geq 2d + 1$) \cite{turn0search8}. Это порождает необходимость понижения размерности реконструированного аттрактора при сохранении всей информации о динамике. Адекватное снижение размерности позволяет не только уменьшить вычислительные затраты и шумовые искажения, но и выявить скрытую структуру фазового пространства, что важно для последующего анализа, предсказания и управления системой. В задачах понижения размерности часто используются методы нелинейного manifold learning, такие как Diffusion Maps, Isomap, UMAP и другие, которые стремятся сохранить геометрию и локальную структуру данных при отображении в пространство более низкой размерности \cite{turn0search32,turn0search2,turn0search5}.

В данной работе рассматривается задача понижения размерности вложения Такенса. Цель состоит в том, чтобы получить компактное латентное представление реконструированного аттрактора, которое сохраняет всю информацию о динамике системы и может быть использовано для анализа поведения, моделирования и предсказания состояния системы.


\section*{Постановка задачи}

Рассмотрим гладкую динамическую систему, задаваемую потоком 
\(\varphi_t : \mathcal{M} \to \mathcal{M}\) на многообразии состояний \(\mathcal{M}\) 
размерности \(n\), и её фазовый аттрактор \(\mathcal{A}\subset \mathcal{M}\).
Предположим, что динамика наблюдается через функции наблюдения  
\[
h : \mathcal{M} \to \mathbb{R}^{d}
\]
Тогда при дискретизации времени наблюдения имеют вид
\[
s_t = h(\varphi_{t\Delta t}(x_0)) + \eta_t
\]
где \(\eta_t\) обозначает шум измерений.

Для каждого временного ряда строится вложения задержек. При фиксированных задержках \(\tau\)
и размерностях вложения \(m\):

\[
X_t = \big(s_t,\, s_{t-\tau_k},\,\dots,\,s_{t-(m_k-1)\tau_k}\big)
\in \mathbb{R}^{m_k d_k}
\]
Это позволяет получить реконструированное пространсва состояний системы:
\[
\widehat{\mathcal{A}} = \{X_t\} = X
\]

Согласно теореме Такенса и её обобщениям\cite{Deyle2011}, при типичных наблюдениях и достаточно большой
размерности вложения реконструированный аттрактор являются гомеоморфным
фазовому аттрактору \(\mathcal{A}\).
\[
\Phi:\widehat{\mathcal{A}} \to \mathcal{A},
\]
Таким образом, восстановленное пространство состояний содержит в себе всю информацию о динамике, однако его размерность значительно превосходит истинную размерность системы, согласно теореме Такенса \cite{takens1981}. Мы ставим целью понизить размерность реконструированной системы при сохранении всей информации о её динамике.
% \[
% \begin{tikzcd}
% \mathcal{A}_1 \arrow[dr, shift left=0.5ex, rightarrow] 
%   \arrow[dr, shift right=0.5ex, leftarrow] 
% & & &
% \mathcal{A}_2 \arrow[dl, shift left=0.5ex, rightarrow] 
%   \arrow[dl, shift right=0.5ex, leftarrow] \\
% & U \arrow[r, leftrightarrow] &  V & \\
% \end{tikzcd}
% \]

% где латентные пространства \(U\), \(V\) интерпретируется как приближённая реконструкция общего состояния системы. подобные низкоразмерные состояния существуют поскоьку для полной реконструкции фазового пространства системя размерность вложения задержет должна быть больше чем \(d' + 1\) где \(d'\) размерность фазового пространства \cite{Deyle2011}


\section*{Методология}

Существует множество методов понижения размерности, направленных на построение
низкоразмерного латентного представления динамической системы.
В данной работе нас интересуют методы, реализующие принцип автокодировщика, то есть обучение отображений
\[
\mathcal{E}: \widehat{\mathcal{A}} \to \mathcal{Z},
\qquad
\mathcal{D}: \mathcal{Z} \to \widehat{\mathcal{A}},
\]
где латентное пространство \(\mathcal{Z}\) имеет существенно меньшую размерность,
а композиция \(\mathcal{D}\circ\mathcal{E}\) приближает тождественное отображение
на реконструированном аттракторе.

\subsection*{Линейные автокодировщики: SVD и PCA}

Классическим примером автокодировщика является сингулярное разложение (SVD),
эквивалентное методу главных компонент (PCA) для центрированных данных.
Пусть \(X \in \mathbb{R}^{N \times D}\) — матрица задержечных векторов.
Тогда
\[
X = U \Sigma V^\top,
\]
и усечение разложения по первым \(r\) сингулярным значениям задаёт оптимальное
в смысле наименьших квадратов линейное вложение
\[
\mathcal{E}(X) = X V_r,
\qquad
\mathcal{D}(Z) = Z V_r^\top.
\]
Линейные методы широко применяются для анализа динамических систем
(например, Dynamic Mode Decomposition) и служат базовой моделью автокодировщика
\cite{Rowley2009}.

\subsection*{Нелинейные автокодировщики и глубокие нейронные сети}

Для сложных нелинейных аттракторов линейные методы оказываются недостаточными.
Нелинейные автокодировщики используют параметризованные нейронные сети
для задания отображений \(\mathcal{E}\) и \(\mathcal{D}\),
обучаемых минимизацией ошибки реконструкции:
\[
\min_{\theta,\phi} \sum_t
\big\| X_t - \mathcal{D}_\phi(\mathcal{E}_\theta(X_t)) \big\|^2.
\]
Такие модели успешно применяются для понижения размерности вложений Такенса
и восстановления латентных динамических координат
\cite{Champion2019,Lusch2018}.

% \subsection*{Вариационные автокодировщики (VAE)}

% Вариационные автокодировщики вводят вероятностную модель латентного пространства,
% задавая апостериорное распределение
% \[
% q_\theta(z \mid X), \qquad p_\phi(X \mid z),
% \]
% и оптимизируя вариационную нижнюю оценку правдоподобия (ELBO).
% Это позволяет получить сглаженное и структурированное латентное пространство,
% в котором динамика часто аппроксимируется простыми стохастическими моделями.
% VAE активно используются для реконструкции и моделирования
% нелинейных динамических систем \cite{Karl2017}.

\subsection*{Автокодировщики на основе Transformer-архитектур}

Transformer-модели используют механизм самовнимания для кодирования
долгосрочных временных зависимостей в задержечных векторах.
В контексте динамических систем такие архитектуры позволяют строить
латентные состояния, инвариантные к выбору задержек
и устойчивые к шуму наблюдений.
Transformer-автокодировщики особенно эффективны при анализе
высокомерных временных рядов и мультисенсорных наблюдений.

В данной работе используется архитектура автокодировщика на основе Transformer,
реализующая encoder--decoder модель.
На вход encoder подаётся фиксированное временное окно исходного сигнала,
представляющее собой последовательность наблюдений.
Encoder отображает входную последовательность в низкоразмерное латентное
представление, после чего decoder восстанавливает исходное временное окно по соответствующему латентному коду.


\subsection*{Понижение размерности с помощью Graph Neural Diffusion}

В данной работе мы хотим адаптировать модель Graph Neural Diffusion (GRAND) \cite{GRAND} для понижения размерности вложения Такенса. Модель GRAND моделирует поток диффузии на графе и тем самым сглаживает данные, что и является понижением размерности.

Для начала необходимо построить граф. Рассмотрим матрицу расстояний $W$, где элемент матрицы $W_{ij}$ задаёт меру расстояния между объектами. В данной работе предлагается рассматривать расстояние как $W_{ij} = \exp(|X_i - X_j|^2 / \epsilon), \ X_i, X_j \in X$, где $X_i, \ X_j$ соответствуют разным временным меткам сигнала. Таким образом мы построили граф $G = (V, \ E)$, где $V$ — это все записи сигнала, которые у нас есть, а $E$ — это все возможные связи между ними.

Для предотвращения потока диффузии между дальними вершинами разрядим граф следующим образом: рассмотрим граф $G' = (V, \ E')$, где $E' = \{(i, \ j) \ | \ W_{ij} > \rho \}$. Таким образом мы описали граф с $n$ вершинами и $e$ рёбрами.

Рассматривается следующее дискретное уравнение диффузии на графе:
$$
\frac{\partial X_i(t)}{\partial t} = \sum_{j  :  (i, j) \ \in  E'
} a(X_i(t), \ X_j(t))(X_j(t) - X_i(t)),
\quad a(X_i, \ X_j) = softmax \left( \frac{(W_KX_i)^\top W_QX_j}{d_k} \right)
$$

Можно заметить что производная ноды по времени это, взвешенная сумма градиентов соседей, таким образом модель сама решает в какую сторону выполнять сглаживания.

Теперь мы можем записать итоговый образ ноды:
$$
X_i(T) = X_i(0) + \int_{0}^T \frac{\partial X(t)}{\partial t} dt, \quad X_i(0) = X_i
$$
Данная постановка удовлетворяет условиям теоремы Пикара–Линделёфа \cite{GRAND}, поэтому существует обратный поток диффузии, который также можно моделировать и воспринимать как декодер, что завершает архитектуру нашего автокодировщика.

Данный этап является лишь упрощением внутренней геометрии реконструированного аттрактора системы и не понижает размерность напрямую. Для финального понижения можно использовать линейный метод, например SVD. Подобные нелинейные методы manifold learning необходимы, поскольку внутренняя геометрия многообразия может быть сильно изогнута, что не улавливается обычными линейными методами. Например, можно рассмотреть одномерную спираль, закрученную на плоскости.


\section*{Эксперимент}

Возьмем данный из \cite{Malekzadeh2018ProtectingSD}. Для начала сгладим сигнал при помощи гаусовского фильтра, после, после построим вложение Такенса с параметрами $\tau = 4, \ m = 15$ и применим модели SVD, GRAND. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{SVD.png}
    \caption{SVD}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{GRAND.png}
    \caption{GRAND}
    \label{fig:my_label}
\end{figure}


К сожалению конкретно эти данные, не могут показать всей силы GRAND полкольку на них даже линейная модель показывает довольно маленькую ошибку реконструкции. Однако, даже на них модель GRAND показала меньшую ошибку.


\newpage

\bibliographystyle{plain}
\bibliography{refs}


\end{document}